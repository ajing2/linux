*** nginx内核参数优化
**** nginx内核参数优化

     - net.ipv4.tcp_max_tw_buckets = 6000 :: timewait 的数量，默认是180000
     - net.ipv4.ip_local_port_range = 1024 65000 :: 允许系统打开的端口范围
     - net.ipv4.tcp_tw_recycle = 1 :: 启用timewait 快速回收
     - net.ipv4.tcp_tw_reuse = 1 :: 开启重用。允许将TIME-WAIT sockets 重新用于新的TCP 连接
     - net.ipv4.tcp_syncookies = 1 :: 开启SYN Cookies，当出现SYN 等待队列溢出时，启用cookies 来处理
     - net.core.somaxconn = 262144 :: web 应用中listen 函数的backlog 默认会给我们内核参数的net.core.somaxconn限制到128，而nginx 定义的NGX_LISTEN_BACKLOG 默认为511，所以有必要调整这个值
     - net.core.netdev_max_backlog = 262144 :: 每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目
     - net.ipv4.tcp_max_orphans = 262144 :: 系统中最多有多少个TCP套接字不被关联到任何一个用户文件句柄上。如果超过这个数字，孤儿连接将即刻被复位并打印出警告信息。这个限制仅仅是为了防止简单的DoS攻击，不能过分依靠它或者人为地减小这个值，更应该增加这个值(如果增加了内存之后)
     - net.ipv4.tcp_max_syn_backlog = 262144 :: 记录的那些尚未收到客户端确认信息的连接请求的最大值。对于有128M内存的系统而言，缺省值是1024，小内存的系统则是128
     - net.ipv4.tcp_timestamps = 0 :: 时间戳可以避免序列号的卷绕。一个1Gbps的链路肯定会遇到以前用过的序列号。时间戳能够让内核接受这种“异常”的数据包。这里需要将其关掉;
     - net.ipv4.tcp_synack_retries = 1 :: 为了打开对端的连接，内核需要发送一个SYN 并附带一个回应前面一个SYN的ACK。也就是所谓三次握手中的第二次握手。这个设置决定了内核放弃连接之前发送SYN+ACK 包的数量
     - net.ipv4.tcp_syn_retries = 1 :: 在内核放弃建立连接之前发送SYN 包的数量;
     - net.ipv4.tcp_fin_timeout = 1 :: 如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间。对端可以出错并永远不关闭连接，甚至意外当机。缺省值是60 秒。2.2 内核的通常值是180秒，3你可以按这个设置，但要记住的是，即使你的机器是一个轻载的WEB 服务器，也有因为大量的死套接字而内存溢出的风险，FIN-WAIT-2 的危险性比FIN-WAIT-1 要小，因为它最多只能吃掉1.5K 内存，但是它们的生存期长些
     - net.ipv4.tcp_keepalive_time = 30 :: 当keepalive 起用的时候，TCP 发送keepalive 消息的频度。缺省是2 小时;
**** 内核参数修改

     - 将/proc/sys中的文件转换成sysctl中的变量依据下面两个简单的规则:

       - 去掉前面部分/proc/sys

       - 将文件名中的斜杠变为点

       例如:

       /proc/sys/net/ipv4/ip_forward => net.ipv4.ip_forward

       /proc/sys/kernel/hostname => kernel.hostname

     - nginx的web连接中出现过多的TIME_WAIT[1]过多，最终会把nginx给脱挂

       #+BEGIN_QUOTE
       查看内核参数, 把内核参数改成: =shell> sysctl net.ipv4.tcp_timestamps=1=

       #net.ipv4.tcp_timestamps 开启时，net.ipv4.tcp_tw_recycle开启才能生效

       =shell> sysctl net.ipv4.tcp_tw_recycle=1=

       #表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭;

       这个变量和时间戳有关，lvs的nat模式中，在转发请求的时候，会把请求中的源ip和目标ip改变，时间戳可能会变，所以就会被回收，但是dr模式，时间戳是不会变的，所以dr模式开启没有问题;

       =shell> sysctl net.ipv4.tcp_tw_reuse=1=

       #表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；该文件表示是否允许重新应用处于TIME-WAIT状态的socket用于新的TCP连接(这个对快速重启动某些服务,而启动后提示端口已经被使用的情形非常有帮助)

       net.ipv4.tcp_tw_recycle依赖net.ipv4.tcp_timestamps 如果后者开启，前者才会生效

       也可以vim /etc/sysctl.conf ,最后sysctl -p就可以生效了
       #+END_QUOTE
**** 下面贴一个完整的内核优化设置

     vi /etc/sysctl.confCentOS5.5中可以将所有内容清空直接替换为如下内容

     #+BEGIN_EXAMPLE
     net.ipv4.ip_forward = 0
     net.ipv4.conf.default.rp_filter = 1
     net.ipv4.conf.default.accept_source_route = 0
     kernel.sysrq = 0
     kernel.core_uses_pid = 1
     net.ipv4.tcp_syncookies = 1
     kernel.msgmnb = 65536
     kernel.msgmax = 65536
     kernel.shmmax = 68719476736
     kernel.shmall = 4294967296
     net.ipv4.tcp_max_tw_buckets = 6000
     net.ipv4.tcp_sack = 1
     net.ipv4.tcp_window_scaling = 1
     net.ipv4.tcp_rmem = 4096 87380 4194304
     net.ipv4.tcp_wmem = 4096 16384 4194304
     net.core.wmem_default = 8388608
     net.core.rmem_default = 8388608
     net.core.rmem_max = 16777216
     net.core.wmem_max = 16777216
     net.core.netdev_max_backlog = 262144
     net.core.somaxconn = 262144
     net.ipv4.tcp_max_orphans = 3276800
     net.ipv4.tcp_max_syn_backlog = 262144
     net.ipv4.tcp_timestamps = 0
     net.ipv4.tcp_synack_retries = 1
     net.ipv4.tcp_syn_retries = 1
     net.ipv4.tcp_tw_recycle = 1
     net.ipv4.tcp_tw_reuse = 1
     net.ipv4.tcp_mem = 94500000 915000000 927000000
     net.ipv4.tcp_fin_timeout = 1
     net.ipv4.tcp_keepalive_time = 30
     net.ipv4.ip_local_port_range = 1024 65000
     #+END_EXAMPLE
     使配置立即生效可使用如下命令: =/sbin/sysctl -p=
**** 下面是关于系统连接数的优化

     linux 默认值 open files 和 max user processes 为1024
     #+BEGIN_EXAMPLE
     #ulimit -n
     1024

     #ulimit –u
     1024
     #+END_EXAMPLE

     *问题描述:* 说明 server 只允许同时打开 1024 个文件，处理 1024个用户进程

     使用ulimit -a 可以查看当前系统的所有限制值，使用ulimit -n 可以查看当前的最大打开文件数;

     新装的linux 默认只有1024 ，当作负载较大的服务器时，很容易遇到error: too many open files。因此，需要将其改大;

     *解决方法:*

     使用 ulimit –n 65535 可即时修改，但重启后就无效了。(注ulimit -SHn 65535 等效 ulimit-n 65535 ，-S 指soft ，-H 指hard)

     有如下三种修改方式:

     1. 在/etc/rc.local 中增加一行 ulimit -SHn 65535
     2. 在/etc/profile 中增加一行 ulimit -SHn 65535
     3. 在/etc/security/limits.conf最后增加:

	#+BEGIN_EXAMPLE
	* soft nofile 65535
        * hard nofile 65535
        * soft nproc 65535
        * hard nproc 65535
	#+END_EXAMPLE


     具体使用哪种，在 CentOS 中使用第1 种方式无效果，使用第3 种方式有效果，而在Debian 中使用第2种有效果;

     备注:ulimit 命令本身就有分软硬设置，加-H 就是硬，加-S 就是软, 默认显示的是软限制;

          soft 限制指的是当前系统生效的设置值。 hard 限制值可以被普通用户降低。但是不能增加。 soft 限制不能设置的比hard 限制更高。 只有 root 用户才能够增加 hard 限制值;
**** 关于FastCGI 的几个指令

     #+BEGIN_QUOTE
     - =fastcgi_cache_path /usr/local/nginx/fastcgi_cache levels=1:2keys_zone=TEST:10minactive=5m;= :: 这个指令为FastCGI 缓存指定一个路径，目录结构等级，关键字区域存储时间和非活动删除时间;
     - =fastcgi_connect_timeout 300;= :: 指定连接到后端FastCGI 的超时时间;
     - =fastcgi_send_timeout 300;= :: 向FastCGI 传送请求的超时时间，这个值是指已经完成两次握手后向FastCGI 传送请求的超时时间;
     - =fastcgi_read_timeout 300;= :: 接收FastCGI 应答的超时时间，这个值是指已经完成两次握手后接收FastCGI 应答的超时时间;
     - =fastcgi_buffer_size 4k;= :: 指定读取FastCGI应答第一部分需要用多大的缓冲区，一般第一部分应答不会超过1k，由于页面大小为4k，所以这里设置为4k;
     - =fastcgi_buffers 8 4k;= :: 指定本地需要用多少和多大的缓冲区来缓冲FastCGI 的应答;
     - =fastcgi_busy_buffers_size 8k;= :: 这个指令我也不知道是做什么用，只知道默认值是fastcgi_buffers 的两倍;
     - =fastcgi_temp_file_write_size 8k;= :: 在写入fastcgi_temp_path 时将用多大的数据块，默认值是fastcgi_buffers 的两倍;
     - =fastcgi_cache TEST= :: 开启FastCGI 缓存并且为其制定一个名称。个人感觉开启缓存非常有用，可以有效降低CPU 负载，并且防止502 错误;
     - =fastcgi_cache_valid 200 302 1h;= :: fastcgi_cache_valid 301 1d;
     - =fastcgi_cache_valid any 1m;= :: 为指定的应答代码指定缓存时间，如上例中将200，302 应答缓存一小时，301 应答缓存1 天，其他为1 分钟;
     - =fastcgi_cache_min_uses 1;= :: 缓存在fastcgi_cache_path 指令inactive 参数值时间内的最少使用次数，如上例，如果在5 分钟内某文件1次也没有被使用，那么这个文件将被移除;
     - =fastcgi_cache_use_stale error timeout invalid_headerhttp_500;= :: 不知道这个参数的作用，猜想应该是让nginx 知道哪些类型的缓存是没用的。以上为nginx 中FastCGI相关参数，另外，FastCGI 自身也有一些配置需要进行优化，如果你使用php-fpm来管理FastCGI，可以修改配置文件中的以下值:
	  #+BEGIN_EXAMPLE
	  60</value>
	  同时处理的并发请求数，即它将开启最多60 个子线程来处理并发连接。
	  102400</value>
	  最多打开文件数。
	  204800</value>
	  每个进程在重置之前能够执行的最多请求数。 
	  #+END_EXAMPLE
     #+END_QUOTE
* Footnotes

[1] 主动关闭的一方在发送最后一个 ack 后, 就会进入 TIME_WAIT 状态 停留2MSL（max segment lifetime）时间, 这个是TCP/IP必不可少的，也就是“解决”不了的; 也就是TCP/IP设计者本来是这么设计的, 主要有两个原因:

    1. 防止上一次连接中的包，迷路后重新出现，影响新连接(经过2MSL，上一次连接中所有的重复包都会消失)
    2. 可靠的关闭TCP连接

       在主动关闭方发送的最后一个 ack(fin) ，有可能丢失，这时被动方会重新发fin, 如果这时主动方处于 CLOSED 状态 ，就会响应 rst 而不是 ack。所以主动方要处于 TIME_WAIT 状态，而不能是 CLOSED;

